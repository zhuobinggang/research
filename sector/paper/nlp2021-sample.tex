%#!platex
\documentclass[
  platex, dvipdfmx,  % ワークフローは必ず明示的に指定する
]{nlp2021}
%#!uplatex
%\documentclass[uplatex,dvipdfmx]{nlp2021}
%#!lualatex
%\documentclass[lualatex]{nlp2021}


% パッケージ
\usepackage{xcolor}  %
\usepackage{graphicx}  % グラフィックス関連
\usepackage{pxrubrica}        % ルビ
\usepackage{url}
\usepackage[square,numbers]{natbib} % 参考文献のフォントサイズを変更する用


% 著者用マクロをここに入れる
%\newcommand{\pkg}[1]{\textsf{#1}}
%\newcommand{\code}[1]{\texttt{#1}}
%\newcommand{\comment}[1]{\textcolor{red}{#1}}
%%%%%%%

\title{補助損失を利用したBERTによる段落分割}
\author{%
  Zhuo Binggang \\ 鳥取大学自然言語処理研究室 \\ \texttt{zhuobinggang@gmail.com}\and
  言語花子 \\ 言語大学処理学部 \\ \texttt{hanako@example.com}}

\begin{document}

\maketitle

\section{はじめに}
段落分割は、トピックセグメンテーションと同様に、テキストセグメンテーションタスクに属する。段落分割はトピックセグメンテーションと非常に似ているが、前者が著者の個人的な判断に大きく依存するためより困難である。しかし、両者の本質が非常に似ているため、本研究はトピックセグメンテーションの先行研究からインスピレーションを得た。

自動段落分割の有用性について、最も直感的な使用シナリオは、無秩序にフォーマットされたWebテキストを手に入れたとき、自動段落分割で人間が資料をよりよく理解できる。 しかし本研究では、自動段落分割は主に手法の有効性を検証するために使用される。

本研究は飯倉らの研究に基づいて補助損失を導入し、パフォーマンスの向上に成功した。この改良の出発点は、近くに分割点があるかどうかを知ることは、現在の判断に重要であるという直感である。たとえば、隣に段落分割点がある場合、現在の位置が分割点である可能性が低いと思われる。モデルが近くに分割点があるかどうかを明示的に理解できるようにするために、BERTアーキテクチャを改善し、補助損失を導入した。実験結果により、この方法が効果的であることを証明できる。

\section{先行研究}

近年のNLP分野におけるBERTの成果により、先行研究をBERTに依存する研究とBERTに依存しない研究の２種類に分類できる。トピックセグメンテーションについて、BERTに依存しない研究の中で優れた結果を出したのは、Bi-LSTM（Koshorek et al、2018）、SEGBOT（Li et al、2018）、Bi-LSTM + CRF（Wang et al、2018）などの手法がある。BERTを使用した研究ではLukasikらcross-segmentationという手法で最先端の成果を出した。

ピックセグメンテーションと比べて段落分割に関する先行研究は少ない。その中で飯倉らの研究は近年比較的良い結果を出した。飯倉らはBERTの上でFocal Lossを導入し、データの不均衡の問題を緩和し、普通のBERTの使い方より優れた結果を出した。

本研究の提案する手法もBERTに基づいている。Lukasikらの研究と同様に、本研究もセグメンテーションポイント付近のローカル情報に焦点を当てる。飯倉らの研究に基づいて、本研究はさらに補助損失を導入し、パフォーマンスの向上に成功した。

飯倉らのデータセットは夏目漱石の小説であり、本研究は毎日新聞を使用する。毎日新聞の記事がさまざまな分野をカバーしており、モデルの汎用性を向上できる。

\section{データセット}

\begin{table*}[t]
  \centering
  \small
  \caption{データセット}
  \begin{tabular}{lcccc}
  \hline
  \     & 記事の数 & 文の数   & 記事の平均段落数 & 段落の平均文数 \\
  \hline
  Train & 2000 & 29436 & 5.30        & 2.78       \\
  Test  & 500  & 7369  & 5.47        & 2.70       \\
  Dev   & 500  & 7139  & 5.15        & 2.77       \\
  \hline
  \label{dataset}
  \end{tabular}
  \end{table*}


飯倉らのデータセットは夏目漱石の小説であり、本研究は2019年の毎日新聞をデータセットとして使用している。毎日新聞の記事がさまざまな分野をカバーしており、モデルの汎用性を向上できる。詳細情報を表\ref{dataset}に示す。

% 参考文献
%\bibliographystyle{junsrt}
%\bibliography{j_yourrefs} % ファイル名は適宜自分のbibファイルに置き換える

%%%%  ここまでが本文+参考文献　5ページ以内


% 付録(Appendix)
% 付録を付けない場合は、以下\end{document}以外を全てをコメントアウトする．
% 本文、参考文献に続けて作成する場合は、必ず \clearpage して新たなページとする

\section{研究手法}

下記すべての手法の入力単位は文である。本研究はwindows sizeを4に固定する。そのため、理想の入力は前後連続の4文である。

\subsection{BERT + BCE Loss}

BERT + BCE Lossは、BERTで文書分類の一般的なやり方で、簡単に段落分割に適用できる。先行研究と同じように、本研究はBERT+BCE Lossをbaselineとして使う。

\subsubsection{手順}

\begin{enumerate}
  \item BERTでテキストを埋め込みに変換する。
  \item 埋め込みをMLPに入力する。MLPの最後の活性化関数をsigmoidとすると、pという0と1の間の値を出力として得られる。本研究では、0は分割しない、1は分割すると規定する。それゆえ、pは分割の確率と見られる。
  \item pとラベルでBCE lossを計算する。
  \item lossが減るようにモデルを訓練する。
\end{enumerate}

その流れを図\ref{fig:steps1}に示す。

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{steps1.png}
  \caption{手順(BERT + BCE Loss)}
  \label{fig:steps1}
  \end{figure}


\end{document}
