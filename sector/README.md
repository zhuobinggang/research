## Overview

Setting: 

* BERT: tohoku university whole word bert
* Architecture: Cross Segmentation Attention 128 (From paper: Text Segmentation by Cross Segment Attention)
* Weighting loss: {1: 2, 0: 1}
* Epochs: 2

## Next plan

### #1 BERT结构对比

- [X] #1 CSG 128
  - [ ] 获取实验结果(5 次)
- [ ] #2 CSG 64
- [X] #3 单句子取出[CLS]Token
- [X] #4 单句子CSG 128
- [ ] #5 单句子，同时训练BERT辨认分割点
- [ ] #6 CSG 256:0 (预测分割点)
- [ ] #7 Kuro Bert
- [X] #8 单句子取出[CLS]Token Cat Sentence

#### 分析

1. #1 vs #2: 前后token量对段落分割精度的影响?
2. #3 vs #4: CSG结构(输入平衡性)对BERT精度的影响? (没有影响。。。只是我BERT弄错了)
3. #5 vs #3: 增加限制可以让模型学习更多东西?
4. #5 vs #2#1: 辨认分割点Loss和CSG模型的内在联系?
5. #6 能否预测分割点？
6. #7 vs #1: 不同Bert精度对比
7. #3 vs #8: #8的精度很低，好像根本没有学习到一样。能猜测到几个原因
  * 如果不在句末加SEP token可能会降低精度
  * 学习的epoch数不足
  * 如果让BERT attend到全部token会提高精度(谷歌的BERT-BiLSTM是这样做的，可是也有不错的成绩)

### #2 其他提高精度的小主意

- [ ] #1 将分割点前后部分倒过来训练


## Record

#### Check best epoch to stop for Simplest weight setting = 1:1

It can be epoch = 2

```
>>> losses
[1776.6383453648305, 1294.191233681282, 838.2970449654094, 644.4661880128842, 461.74606151817716]
>>> dics[0]
{'prec': 0.8119891008174387, 'rec': 0.4401772525849335, 'f1': 0.5708812260536399, 'bacc': 0.7122065152575113}
>>> dics[1]
{'prec': 0.7263157894736842, 'rec': 0.5096011816838996, 'f1': 0.5989583333333334, 'bacc': 0.7399502367181207}
>>> dics[2]
{'prec': 0.581081081081081, 'rec': 0.5716395864106352, 'f1': 0.5763216679076694, 'bacc': 0.7539486485857152}
>>> dics[3]
{'prec': 0.5940438871473355, 'rec': 0.5598227474150664, 'f1': 0.5764258555133079, 'bacc': 0.7503248989531353}
>>> dics[4]
{'prec': 0.6963562753036437, 'rec': 0.5081240768094535, 'f1': 0.5875320239111871, 'bacc': 0.7369270144156932}
```

#### #4 Single Sentence CSG

```
# TEST
{'prec': 0.9747368421052631, 'rec': 0.6955433149724587, 'f1': 0.8118059614260665, 'bacc': 0.845824659108728}
{'prec': 0.9632843791722296, 'rec': 0.7225838758137206, 'f1': 0.8257510729613735, 'bacc': 0.8583173570523444}
{'prec': 0.9625850340136054, 'rec': 0.7085628442663996, 'f1': 0.816267666570522, 'bacc': 0.8513068412786838}
{'prec': 0.9061913696060038, 'rec': 0.7255883825738608, 'f1': 0.8058954393770857, 'bacc': 0.8546816980473415}
{'prec': 0.9031668696711328, 'rec': 0.7426139208813219, 'f1': 0.81505908216543, 'bacc': 0.8627077176066966}

# DEV
{'prec': 0.8888888888888888, 'rec': 0.38995568685376664, 'f1': 0.5420944558521561, 'bacc': 0.6912081381492959}
{'prec': 0.8452012383900929, 'rec': 0.4032496307237814, 'f1': 0.546, 'bacc': 0.6959131406988794}
{'prec': 0.782016348773842, 'rec': 0.4239290989660266, 'f1': 0.549808429118774, 'bacc': 0.7028258700221954}
{'prec': 0.684931506849315, 'rec': 0.51698670605613, 'f1': 0.5892255892255892, 'bacc': 0.7401017606131689}
{'prec': 0.6635514018691588, 'rec': 0.5243722304283605, 'f1': 0.5858085808580858, 'bacc': 0.7416240864273399}
```

#### #8 Single Sentence Cat Sentence

```
# TEST
{'prec': 0.15030674846625766, 'rec': 0.09814722083124687, 'f1': 0.11875189336564677, 'bacc': 0.4891493270191929}
{'prec': 0.17886063361272908, 'rec': 0.6304456685027542, 'f1': 0.2786631252766711, 'bacc': 0.5026214280858823}
{'prec': 0.17504273504273504, 'rec': 0.5127691537305958, 'f1': 0.26099146170511023, 'bacc': 0.49537862770358887}
{'prec': 0.16990291262135923, 'rec': 0.05257886830245368, 'f1': 0.08030592734225621, 'bacc': 0.4985447072718326}
{'prec': 0.17504051863857376, 'rec': 0.5948923385077617, 'f1': 0.27049180327868855, 'bacc': 0.49463383826415663}

# DEV
{'prec': 0.12254335260115606, 'rec': 0.15657311669128507, 'f1': 0.13748378728923474, 'bacc': 0.4915833369611326}
{'prec': 0.13571889103803997, 'rec': 0.621861152141802, 'f1': 0.2228102672664726, 'bacc': 0.5046705806402407}
{'prec': 0.13549239920687375, 'rec': 0.6056129985228951, 'f1': 0.22144207399405888, 'bacc': 0.5039716808927018}
{'prec': 0.12716763005780346, 'rec': 0.06499261447562776, 'f1': 0.08602150537634407, 'bacc': 0.49799779227322627}
{'prec': 0.13352112676056338, 'rec': 0.7001477104874446, 'f1': 0.22427253371185238, 'bacc': 0.4986916299752736}

```


