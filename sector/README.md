## Overview

Setting: 

* BERT: tohoku university whole word bert
* Architecture: Cross Segmentation Attention 128 (From paper: Text Segmentation by Cross Segment Attention)
* Weighting loss: {1: 2, 0: 1}
* Epochs: 2

## Next plan

### #1 BERT结构对比

- [X] #1 CSG 128
- [ ] #2 CSG 64
- [X] #3 单句子取出[CLS]Token
- [X] #4 单句子CSG 128
- [ ] #5 单句子，同时训练BERT辨认分割点 (BERTSEGBOT)
  - [X] #5.1 训练方式: 16句, ground truth, 跳过第一句，辨认第一个分割点，约4000行训练集
- [ ] #6 CSG 256:0 (预测分割点)
- [ ] #7 Kuro Bert
- [X] #8 单句子取出[CLS]Token Cat Sentence
- [X] #9 双句子取出[CLS]Token (这个效果最好，以这个为基准线)
- [X] #10 order matters 2 sentences
- [ ] #11 0:2sentences (用下文两句以预测分割点)
- [X] #12 对dataset进行剪枝
- [X] #13 长距离BERT
  - [X] length = 4
  - [X] length = 6 epoch 2
- [X] #14 TF代替GRU
- [X] #14 TF极限探测
  - [X] # length = 3:3, weight = 1:1, head = 4
  - [X] # length = 3:3, weight = 1:1, head = 16
  - [X] # length = 3:3, weight = 1:1, head = 24
  - [X] # length = 3:3, weight = 1:1, head = 32
  - [X] # length = 4:4, weight = 1:1, head = 8
  - [X] # length = 5:5, weight = 1:1, head = 8
  - [X] # length = 6:6, weight = 1:1, head = 8

#### 分析

1. #1 vs #2: 前后token量对段落分割精度的影响?
2. #3 vs #4: CSG结构(输入平衡性)对BERT精度的影响? (没有影响。。。只是我BERT弄错了)
3. #5 vs #3: 增加限制可以让模型学习更多东西?
4. #5 vs #2#1: 辨认分割点Loss和CSG模型的内在联系?
5. #6 能否预测分割点？
6. #7 vs #1: 不同Bert精度对比
7. #3 vs #8: #8的精度很低，好像根本没有学习到一样。能猜测到几个原因
  * 如果不在句末加SEP token可能会降低精度
  * 学习的epoch数不足
  * 如果让BERT attend到全部token会提高精度(谷歌的BERT-BiLSTM是这样做的，可是也有不错的成绩)

### #2 其他提高精度的小主意

- [ ] #1 将分割点前后部分倒过来训练


## Record

#### Check best epoch to stop for Simplest weight setting = 1:1

It can be epoch = 2

```
>>> losses
[1776.6383453648305, 1294.191233681282, 838.2970449654094, 644.4661880128842, 461.74606151817716]
>>> dics[0]
{'prec': 0.8119891008174387, 'rec': 0.4401772525849335, 'f1': 0.5708812260536399, 'bacc': 0.7122065152575113}
>>> dics[1]
{'prec': 0.7263157894736842, 'rec': 0.5096011816838996, 'f1': 0.5989583333333334, 'bacc': 0.7399502367181207}
>>> dics[2]
{'prec': 0.581081081081081, 'rec': 0.5716395864106352, 'f1': 0.5763216679076694, 'bacc': 0.7539486485857152}
>>> dics[3]
{'prec': 0.5940438871473355, 'rec': 0.5598227474150664, 'f1': 0.5764258555133079, 'bacc': 0.7503248989531353}
>>> dics[4]
{'prec': 0.6963562753036437, 'rec': 0.5081240768094535, 'f1': 0.5875320239111871, 'bacc': 0.7369270144156932}
```

#### #4 Single Sentence CSG

```
# TEST
{'prec': 0.9747368421052631, 'rec': 0.6955433149724587, 'f1': 0.8118059614260665, 'bacc': 0.845824659108728}
{'prec': 0.9632843791722296, 'rec': 0.7225838758137206, 'f1': 0.8257510729613735, 'bacc': 0.8583173570523444}
{'prec': 0.9625850340136054, 'rec': 0.7085628442663996, 'f1': 0.816267666570522, 'bacc': 0.8513068412786838}
{'prec': 0.9061913696060038, 'rec': 0.7255883825738608, 'f1': 0.8058954393770857, 'bacc': 0.8546816980473415}
{'prec': 0.9031668696711328, 'rec': 0.7426139208813219, 'f1': 0.81505908216543, 'bacc': 0.8627077176066966}

# DEV
{'prec': 0.8888888888888888, 'rec': 0.38995568685376664, 'f1': 0.5420944558521561, 'bacc': 0.6912081381492959}
{'prec': 0.8452012383900929, 'rec': 0.4032496307237814, 'f1': 0.546, 'bacc': 0.6959131406988794}
{'prec': 0.782016348773842, 'rec': 0.4239290989660266, 'f1': 0.549808429118774, 'bacc': 0.7028258700221954}
{'prec': 0.684931506849315, 'rec': 0.51698670605613, 'f1': 0.5892255892255892, 'bacc': 0.7401017606131689}
{'prec': 0.6635514018691588, 'rec': 0.5243722304283605, 'f1': 0.5858085808580858, 'bacc': 0.7416240864273399}
```

#### #8 Single Sentence Cat Sentence

```
# TEST
{'prec': 0.15030674846625766, 'rec': 0.09814722083124687, 'f1': 0.11875189336564677, 'bacc': 0.4891493270191929}
{'prec': 0.17886063361272908, 'rec': 0.6304456685027542, 'f1': 0.2786631252766711, 'bacc': 0.5026214280858823}
{'prec': 0.17504273504273504, 'rec': 0.5127691537305958, 'f1': 0.26099146170511023, 'bacc': 0.49537862770358887}
{'prec': 0.16990291262135923, 'rec': 0.05257886830245368, 'f1': 0.08030592734225621, 'bacc': 0.4985447072718326}
{'prec': 0.17504051863857376, 'rec': 0.5948923385077617, 'f1': 0.27049180327868855, 'bacc': 0.49463383826415663}

# DEV
{'prec': 0.12254335260115606, 'rec': 0.15657311669128507, 'f1': 0.13748378728923474, 'bacc': 0.4915833369611326}
{'prec': 0.13571889103803997, 'rec': 0.621861152141802, 'f1': 0.2228102672664726, 'bacc': 0.5046705806402407}
{'prec': 0.13549239920687375, 'rec': 0.6056129985228951, 'f1': 0.22144207399405888, 'bacc': 0.5039716808927018}
{'prec': 0.12716763005780346, 'rec': 0.06499261447562776, 'f1': 0.08602150537634407, 'bacc': 0.49799779227322627}
{'prec': 0.13352112676056338, 'rec': 0.7001477104874446, 'f1': 0.22427253371185238, 'bacc': 0.4986916299752736}

```

#### #3 Single Sentence

```
# TEST
{'prec': 0.9117464263517713, 'rec': 0.7346019028542814, 'f1': 0.8136439267886854, 'bacc': 0.8596211244936631}
{'prec': 0.9750865051903114, 'rec': 0.7055583375062594, 'f1': 0.8187100522951772, 'bacc': 0.8508321703756283}
{'prec': 0.9469548133595285, 'rec': 0.7240861291937907, 'f1': 0.8206583427922816, 'bacc': 0.8576623182475174}
{'prec': 0.9468567725210628, 'rec': 0.7315973960941412, 'f1': 0.8254237288135593, 'bacc': 0.8613638684094287}
{'prec': 0.9469895287958116, 'rec': 0.7245868803204807, 'f1': 0.8209929078014184, 'bacc': 0.8579126938108623}

# DEV
{'prec': 0.7799511002444988, 'rec': 0.4711964549483013, 'f1': 0.587476979742173, 'bacc': 0.7253172130807305}
{'prec': 0.8493975903614458, 'rec': 0.41654357459379615, 'f1': 0.5589692765113975, 'bacc': 0.7025601126338868}
{'prec': 0.788659793814433, 'rec': 0.4519940915805022, 'f1': 0.5746478873239437, 'bacc': 0.7166298993429128}
{'prec': 0.7807228915662651, 'rec': 0.47858197932053176, 'f1': 0.5934065934065933, 'bacc': 0.7288957417735855}
{'prec': 0.8242894056847545, 'rec': 0.4711964549483013, 'f1': 0.599624060150376, 'bacc': 0.7278303499324554}
```

#### #10 Order matters 2 sentences

```
# TEST
{'prec': 0.636737491432488, 'rec': 0.4651977966950426, 'f1': 0.5376157407407408, 'bacc': 0.7039347555676403}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}
{'prec': 0.7162750217580505, 'rec': 0.41211817726589883, 'f1': 0.523204068658614, 'bacc': 0.6884279366589094}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}

# DEV
{'prec': 0.603125, 'rec': 0.28508124076809455, 'f1': 0.38716148445336007, 'bacc': 0.6280329667399989}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}
{'prec': 0.7348837209302326, 'rec': 0.23338257016248154, 'f1': 0.3542600896860986, 'bacc': 0.6101799759654081}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}
```


#### #9 Double Sentence
```
# TEST (avg f1 0.8261207760292912)
{'prec': 0.9512195121951219, 'rec': 0.7225838758137206, 'f1': 0.8212862834376778, 'bacc': 0.8572897745753297}
{'prec': 0.9569190600522193, 'rec': 0.7341011517275914, 'f1': 0.8308302635307453, 'bacc': 0.8634810788383765}
{'prec': 0.9030159668835009, 'rec': 0.7646469704556835, 'f1': 0.8280911062906724, 'bacc': 0.8734538259525578}
{'prec': 0.9068360556563824, 'rec': 0.7506259389083625, 'f1': 0.8213698630136986, 'bacc': 0.8669841430615366}
{'prec': 0.8658669574700109, 'rec': 0.7951927891837757, 'f1': 0.8290263638736621, 'bacc': 0.884291905678962}

# DEV
{'prec': 0.7876344086021505, 'rec': 0.4327917282127031, 'f1': 0.5586272640610105, 'bacc': 0.7073714181387939}
{'prec': 0.8078947368421052, 'rec': 0.4534711964549483, 'f1': 0.5808893093661306, 'bacc': 0.7183965532194778}
{'prec': 0.6618705035971223, 'rec': 0.5435745937961596, 'f1': 0.5969180859691808, 'bacc': 0.7503114001651576}
{'prec': 0.6763005780346821, 'rec': 0.518463810930576, 'f1': 0.5869565217391304, 'bacc': 0.7400406785975704}
{'prec': 0.6515426497277677, 'rec': 0.5302806499261448, 'f1': 0.5846905537459284, 'bacc': 0.7432074942571094}

```

#### #1 CSG 128
```
# TEST 
{'prec': 0.7659774436090225, 'rec': 0.40811216825237856, 'f1': 0.5325057170859196, 'bacc': 0.6905893453484716}
{'prec': 0.7827975673327541, 'rec': 0.4511767651477216, 'f1': 0.5724269377382465, 'bacc': 0.7120675605078792}
{'prec': 0.7938871473354232, 'rec': 0.5072608913370055, 'f1': 0.6190039718912312, 'bacc': 0.73940654085509}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}
# DEV
{'prec': 0.5817174515235457, 'rec': 0.310192023633678, 'f1': 0.4046242774566474, 'bacc': 0.6378467543345452}
{'prec': 0.5608695652173913, 'rec': 0.3810930576070901, 'f1': 0.45382585751978893, 'bacc': 0.6674713631649798}
{'prec': 0.551789077212806, 'rec': 0.4327917282127031, 'f1': 0.48509933774834435, 'bacc': 0.6892082927104183}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}
{'prec': 0, 'rec': 0.0, 'f1': 0, 'bacc': 0.5}

# 一开始的结果
test dataset:
prec = 0.8530954879328436
rec = 0.814221331997997
f1 = 0.8332052267486548

dev dataset:
prec = 0.856694560669456
rec = 0.8202303455182774
!f1 = 0.8380660015349195
```

#### #12 对dataset进行剪枝

因为正解总体的比率高了，所以回归率比较高, 但是精度相应的下降了, 说明大部分是盲猜。相比起来正常训练情况下0.95的精度，0.75的回归率，肯定是没法比的。

```
#TEST
{'prec': 0.5583918315252074, 'rec': 0.8763144717075614, 'f1': 0.6821282401091405, 'bacc': 0.8633059648965065}
#DEV
{'prec': 0.3356643356643357, 'rec': 0.7799113737075333, 'f1': 0.4693333333333333, 'bacc': 0.7705816863968327}
```

#### #5.1 训练方式: 16句, ground truth, 跳过第一句，辨认第一个分割点，约4000行训练集 

回归率低的原因是: 16句里边基本上会包含一个以上分割点，所以跳过第一个分割点也是情理之中，所以回归率很低。但总的来说，有学习到了

```
#TEST
{'prec': 0.7840065952184666, 'rec': 0.47597597597597596, 'f1': 0.592338835253815, 'bacc': 0.7238181664628393}
#DEV
{'prec': 0.4975288303130148, 'rec': 0.44542772861356933, 'f1': 0.47003891050583657, 'bacc': 0.6878726488624164}
```


#### #13 长距离BERT

BASELINE: length = 4

```
>>> testdic
{'prec': 0.9021803182086034, 'rec': 0.7662662662662663, 'f1': 0.8286874154262518, 'bacc': 0.8741553072813213}
>>> devdic
{'prec': 0.8066298342541437, 'rec': 0.4306784660766962, 'f1': 0.5615384615384615, 'bacc': 0.7073428885101324}
```

length = 6
```
>>> G['testdic0']
{'prec': 0.9333764553686934, 'rec': 0.7222222222222222, 'f1': 0.814334085778781, 'bacc': 0.8555405324199267}
>>> G['testdic1']
{'prec': 0.9286616161616161, 'rec': 0.7362362362362362, 'f1': 0.8213288665549973, 'bacc': 0.8620067065442945}
>>> G['devdic0']
{'prec': 0.7610619469026548, 'rec': 0.5073746312684366, 'f1': 0.608849557522124, 'bacc': 0.7413500983621142}
>>> G['devdic1']
{'prec': 0.7391304347826086, 'rec': 0.47640117994100295, 'f1': 0.5793721973094171, 'bacc': 0.725177971738836}
```

#### #14 TF

1. 我本来以为TF会掉链子，结果发现8个头的TF效果喜人，以后可以继续延用这个
2. pos大概会增加100秒的耗时，不是什么大问题

```
# 0 Weight = 1: 3, length = 2: 2, head = 1
>>> G['testdic0']
{'prec': 0.9413012729844413, 'rec': 0.6661661661661662, 'f1': 0.7801875732708089, 'bacc': 0.8285941701571772}
>>> G['devdic0']
{'prec': 0.8221574344023324, 'rec': 0.415929203539823, 'f1': 0.5523996082272282, 'bacc': 0.7009963586810378}

# 1 Weight = 1: 1, length = 3: 3, head = 1, with pos
{'prec': 0.8072368421052631, 'rec': 0.6141141141141141, 'f1': 0.6975554292211483, 'bacc': 0.7912106535957266}
# 2 Weight = 1: 1, length = 3: 3, head = 1, no pos
{'prec': 0.8035168195718655, 'rec': 0.526026026026026, 'f1': 0.6358136721113128, 'bacc': 0.7491136079291839}
# 3 Weight = 1: 1, length = 3: 3, head = 8, with pos
{'prec': 0.9179926560587516, 'rec': 0.7507507507507507, 'f1': 0.8259911894273129, 'bacc': 0.8681282147480092}
# 4 Weight = 1: 1, length = 3: 3, head = 8, no pos
{'prec': 0.7260869565217392, 'rec': 0.49262536873156343, 'f1': 0.5869947275922671, 'bacc': 0.7319192642149935}
```

```
# length = 3:3, weight = 1:1, head = 4
# length = 3:3, weight = 1:1, head = 16
# length = 3:3, weight = 1:1, head = 24
# length = 3:3, weight = 1:1, head = 32
# length = 3:3, weight = 1:1, head = 8, dropout = 0.1
# length = 4:4, weight = 1:1, head = 8
# length = 5:5, weight = 1:1, head = 8
```


