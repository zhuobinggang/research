## Done
* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* モジュール化(12/22)
* Backpropagation(誤差逆伝播法)の流れを図示(12/22)
* Hello worldプロジェクトの訓練(12/23)

## TODO
- [X] #1 Hello worldプロジェクトに基づき、文章分割のプロジェクトを構築する (2020/12/28)
  - [X] #1.1 Dataset处理，输入text输出全部sentences转换成的embeddings, 函数名就叫embeddings from text
  - [X] #1.2 改装pointer network, 目标是(输入text，输出一个index)
  - [X] #1.3 根据#1.2改装训练函数, 目标是重复训练之后能正确输出index
    - [X] 伪dataset： [A,B,'C/','D/',E,F,'G/',H,I,J,'K/'] ，正确输出: [3,4,7,11]
    - [X]  # 计算 [seq len] & [onehot] 的loss
- [ ] #2 架构优化
  - [ ] # 2.1 Transformerのアーキテクチャを使用してデコーダを最適化する
  - [ ] # 2.2 考虑如何避免重复输出指针的问题
  - [ ] # 2.3 尝试生成数据的时候限制不能生成重复
  - [ ] # 2.4 尝试生成数据的时候，将数值调大
  - [ ] # 2.5 尝试增加数据量(1500, 2000, 3000)
- [ ] #3 处理wikisection dataset, 调用read data函数，输入文件名，输出[(sentences,correct indexs)]

## Consideration
现在在做#1，因为一开始就设想到转换的麻烦，所以用一个Embedding层把耦合都隔断了，所以现在的主要难点在哪里？
1. 首先是文章太长，一共200多个输入，怕是LSTM也有点难顶

解决方案：
1. 考虑使用LongFormer. 总之先用200多个输入试试再说

