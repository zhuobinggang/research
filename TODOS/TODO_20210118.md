## Done

* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* Hello worldプロジェクトの訓練(12/23)
* 文章分割モデルを構築 (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* (Hello world project)結果セクションの示したように、set2seqには順序がとても重要(Order matters)であることを証明した(2021/1/8)
* 研究の調整：ポインタネットワークを破棄、双方向LSTMを使用する(新規性が減った)(2021/1/11)
* マルチポインタネットワークの提案と実験(2021/1/18)


## TODO

- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319)
  - [ ] VAE的论文，检讨怎么用到NLP上
  - [X] Order matters
- [ ] 实验
  - [X] MSE Loss (可以优化，又一次花费15步左右将loss从20降到5; 可以优化但是似乎会遇上局部最优解)
  - [X] 正常BCE LOSS + 一层缓冲层 (BCE sum的情况，先是下降了，然后又上升，然后爆表； mean的情况也一样，没有效果)
  - [X] MSE Loss + 一层缓冲层 (会遇上清零，说明加一层缓冲跟不加没什么差别)
  - [X] Adam + MSE (虽然有点花时间，但是Adam每一步几乎都成功在下降)
  - [X] Adam + BCE (虽然有点花时间，但是Adam每一步几乎都成功在下降)
  - [X] SGD + 0.01 learning rate + BCE (可以, 果然只是对学习率敏感而已)
- [X] 制作数字clustering训练集


## 思考




