## Done

* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* Hello worldプロジェクトの訓練(12/23)
* 文章分割モデルを構築 (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* (Hello world project)結果セクションの示したように、set2seqには順序がとても重要(Order matters)であることを証明した(2021/1/8)
* 研究の調整：ポインタネットワークを破棄、双方向LSTMを使用する(新規性が減った)(2021/1/11)
* マルチポインタネットワークの提案と実験(2021/1/18)


## TODO

- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319)
  - [ ] VAE的论文，检讨怎么用到NLP上
  - [X] Order matters
- [X] 实验
  - [X] MSE Loss (可以优化，又一次花费15步左右将loss从20降到5; 可以优化但是似乎会遇上局部最优解)
  - [X] 正常BCE LOSS + 一层缓冲层 (BCE sum的情况，先是下降了，然后又上升，然后爆表； mean的情况也一样，没有效果)
  - [X] MSE Loss + 一层缓冲层 (会遇上清零，说明加一层缓冲跟不加没什么差别)
  - [X] Adam + MSE (虽然有点花时间，但是Adam每一步几乎都成功在下降)
  - [X] Adam + BCE (虽然有点花时间，但是Adam每一步几乎都成功在下降)
  - [X] SGD + 0.01 learning rate + BCE (可以, 果然只是对学习率敏感而已)
- [X] 制作数字clustering训练集
- [X] 进行文本clustering
- [X] 乍看之下精度并不高，思索原因，
  - 1）太长的文章，不相关的句子太多，所以大部分的句子都只能将自己的分数弄得很低(sigmoid(0) = 0.5，所以可以得知对于大多数输入，query清零了)，为了抗衡，考虑在训练时不要乘以对角线清零矩阵
  - 2）对于有序输入来说不需要attend过广范围的输入item，这样会导致weight degeneration; 当我们要求model给我们分段的时候，我们应当拥有一个前置知识： 为了分段我应当注目多广的范围，这不能算是作弊
  - 3) 因为使用了transformer类似的架构，所以不能主动遗忘过长的句子（或者底层的BiLSTM已经考虑到了但是效果没有那么好），能否再使用一个BiLSTM来替我完成attention的任务?
- [X] 编码沙漏模型
- [X] 先用helloworld项目来实验, 不乘以对角线清零矩阵(可以，但是优化的速度低了)
- [ ] 晚上回去之前重新用数据集训练，设定epoch = 20 
  - [X] 训练没有diagonal清零的策略 (Doing)
  - [ ] 训练沙漏模型 (TODO)
- [ ] 获得实验结果数据
  - [X] 确认一下sector是怎么做的
  - [X] Pk = 0.3914075914222044 (Jan 23 11:14)
- [X] 决定分割策略: 和上一个集合里所有元素的分数平均值大于平均值 (1.22)
- [X] 编码Pk (1.22)
- [ ] 想办法加速训练
  - [ ] 首先最大的性能损耗应该说来自于s-bert, 所以换成256D，然后我们自己训练(性能可能会下降但是总比之前强)
  - [ ] batch化 (Doing)
  - [X] 测试时间： 93分钟，跑3907个没有算loss居然要93分钟
  - [X] 将sentences batch了，加速了不少，性能大概提高了25% (100个数据的情况,110s -> 80s)
  - [X] 加上了no grad注解, 但是没有什么改变
- [ ] 想办法提高性能
  - [ ] 将输入掉转过来反向训练
  - [ ] 训练沙漏模型
  - [ ] 从头训练s bert
  - [ ] 使用textseg和sector作为baseline
  - [ ] 使用bloom代 s bert


## 发现

1. 较好的学习了“列表通常属于同一个项目”
2. 开头一个没见过的名词，通常意味着新Topic的开始




