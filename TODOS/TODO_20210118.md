## Done

* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* Hello worldプロジェクトの訓練(12/23)
* 文章分割モデルを構築 (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* (Hello world project)結果セクションの示したように、set2seqには順序がとても重要(Order matters)であることを証明した(2021/1/8)
* 研究の調整：ポインタネットワークを破棄、双方向LSTMを使用する(新規性が減った)(2021/1/11)
* マルチポインタネットワークの提案と実験(2021/1/18)
* epoch = 5, Pk = 0.3914075914222044 (Multi-pointer network) (Baseline: 所有地方都不是分割点, Pk = 0.46275868801249515) (Sector: 12.7) (2021/1/23) 
* Batch化 (2021/1/24)
* 预存句子embedding到本地数据库以加快训练速度(3h15min / epoch -> 3min23sec / epoch) (2021/1/25)


## TODO

- [ ] 论文待看
  - [ ] VAE的论文，检讨怎么用到NLP上
  - [X] Order matters
  - [X] Segbot
  - [ ] Text segmentation by cross segment attention
- [X] 实验
  - [X] MSE Loss (可以优化，又一次花费15步左右将loss从20降到5; 可以优化但是似乎会遇上局部最优解)
  - [X] 正常BCE LOSS + 一层缓冲层 (BCE sum的情况，先是下降了，然后又上升，然后爆表； mean的情况也一样，没有效果)
  - [X] MSE Loss + 一层缓冲层 (会遇上清零，说明加一层缓冲跟不加没什么差别)
  - [X] Adam + MSE (虽然有点花时间，但是Adam每一步几乎都成功在下降)
  - [X] Adam + BCE (虽然有点花时间，但是Adam每一步几乎都成功在下降)
  - [X] SGD + 0.01 learning rate + BCE (可以, 果然只是对学习率敏感而已)
- [X] 制作数字clustering训练集
- [X] 进行文本clustering
- [X] 乍看之下精度并不高，思索原因，
  - 1）太长的文章，不相关的句子太多，所以大部分的句子都只能将自己的分数弄得很低(sigmoid(0) = 0.5，所以可以得知对于大多数输入，query清零了)，为了抗衡，考虑在训练时不要乘以对角线清零矩阵
  - 2）对于有序输入来说不需要attend过广范围的输入item，这样会导致weight degeneration; 当我们要求model给我们分段的时候，我们应当拥有一个前置知识： 为了分段我应当注目多广的范围，这不能算是作弊
  - 3) 因为使用了transformer类似的架构，所以不能主动遗忘过长的句子（或者底层的BiLSTM已经考虑到了但是效果没有那么好），能否再使用一个BiLSTM来替我完成attention的任务?
- [X] 编码沙漏模型
- [X] 先用helloworld项目来实验, 不乘以对角线清零矩阵(可以，但是优化的速度低了)
- [ ] 晚上回去之前重新用数据集训练，设定epoch = 20 
  - [X] 训练没有diagonal清零的策略
  - [ ] 训练沙漏模型 (TODO)
- [X] 获得实验结果数据
  - Baseline(所有地方都不是分割点) Pk = 0.46275868801249515
  - [X] 确认一下sector是怎么做的
- [X] 决定分割策略: 和上一个集合里所有元素的分数平均值大于平均值 (1.22)
- [X] 编码Pk (1.22)
- [X] 想办法加速训练
  - 1 epoch 大概3小时15分钟 (Jan 1.24)
  - 1 epoch 3m23s (Jan 1.25)
  - [X] batch化 (BCE的数值太大，要考虑用mean) (因为是sigmoid之后的结果，首先CE是必须的，但是全部mean的话loss又太小，只能耍一点花招了, 单个batch内sum，batch间手动mean一下就好) (Hello world项目，datas length = 900,  epoch=5. batch=4时候24秒，batch=1时候41秒, batch=8 12秒)
  - [X] 测试时间： 93分钟，跑3907个没有算loss居然要93分钟
  - [X] 测试时间： 15分钟 / 1000 datas (将sbert batch之后)
  - [X] 加上了no grad注解, 但是没有什么改变
  - [X] batch+sbert batch化，1 epoch 大概2小时27分 (J 1.25)
  - [X] 将sbert的结果全都存到文件里去
  - [X] 测试时间： 22秒 / 3907个，全部存到文件里去就完了
- [ ] 想办法提高性能
  - [ ] 将输入掉转过来反向训练
  - [ ] 训练沙漏模型
  - [ ] 从头训练s bert
  - [ ] 使用textseg和sector作为baseline (Doing)
  - [ ] 设置Dropout 50%
  - [X] 使用bloom代 s bert(不做，相信sbert)
  - [X] 因为alarm比较多，所以想办法提高分割算法的阈值 (不做了，我那个方法大概是最好的了)
- [ ] 对照实验
  - [X] TextSeg，编码 (Pk = 0.31449481849511074(82119, 8862, 16964))
  - [ ] SegBot，编码 (Doing)
- [ ] 每次训练都输出平均loss用于制图以确定是否过度优化 (Doing)


## 思考

* TextSeg的精度也是到31就停止了，最好从0开始训练然后找到最低点
* 必须思考影响精度的主要原因之一是sentence embedding获取方式, 但是这个跟模型结构基本上可以分离，作为一个模块，所以我们现在得到了对照结果，所以只要得到的实验结果就行了，我们可以对他们的模型做可以想象的优化，但是能否作为我们结果的一部分还存疑
* 决定方针，现在放弃MPN的想法，有两个前进方向，
  * 一是利用MLP来做其他事情(比如单步多输出的什么)
    * 用大规模数据集训练然后用于分割试试？ (大规模训练，难道不是可以实现句子的word2vec?)
  * 二是想办法在文本分割上做文章
    * 提出不对称性, 提高SegBot精度
    * 提出不对称性, 提高textSeg精度
    * 用来作段落分割, 继承前辈的研究 (新颖性在于段落分割+日语) (毕业倒是没有什么问题，可是凭这个能上ACL？)

* Segbot，使用指针网络真的有用吗？值得实验吗？

## 训练结果

### Multi pointer network
* epoch = 5, Pk = 0.3914075914222044 (Jan 23 11:14)
* epoch = 6, Pk = 0.399746192893401 (Jan 24 12:07)
* epoch = 8, batch = 8, Pk = 0.3810445816437122 (Jan 25 17:36) (82119, 14813, 16478)
* epoch ~= 40, batch = 8,  Pk = 0.3457421546779673(82119, 12216, 16176)
* 再往后就没有提高了

### TextSeg
* Pk = 0.35185523447679584
* res = (82119, 15692, 13202)
* Pk = 0.43876569368842777
* res = (82119, 31346, 4685)
* Pk = 0.3509541031917096
* res = (82119, 962, 27858)
* Pk = 0.3111947295997272
* res = (82119, 6748, 18807)
* Pk = 0.31986507385623303
* res = (82119, 7438, 18829)
* Pk = 0.32189870797257636
* res = (82119, 9667, 16767)
* Pk = 0.32262935496048417
* res = (82119, 5332, 21162)
* Pk = 0.3320547011044947
* res = (82119, 6356, 20912)
* Pk = 0.323360001948392
* res = (82119, 2961, 23593)
* Pk = 0.3558372605608933
* res = (82119, 18962, 10259)

