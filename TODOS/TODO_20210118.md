## Done

* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* Hello worldプロジェクトの訓練(12/23)
* 文章分割モデルを構築 (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* (Hello world project)結果セクションの示したように、set2seqには順序がとても重要(Order matters)であることを証明した(2021/1/8)
* 研究の調整：ポインタネットワークを破棄、双方向LSTMを使用する(新規性が減った)(2021/1/11)
* マルチポインタネットワークの提案と実験(2021/1/18)
* epoch = 5, Pk = 0.3914075914222044 (Multi-pointer network) (Baseline: 所有地方都不是分割点, Pk = 0.46275868801249515) (Sector: 12.7) (2021/1/23) 
* Batch化 (2021/1/24)


## TODO

- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319)
  - [ ] VAE的论文，检讨怎么用到NLP上
  - [X] Order matters
- [X] 实验
  - [X] MSE Loss (可以优化，又一次花费15步左右将loss从20降到5; 可以优化但是似乎会遇上局部最优解)
  - [X] 正常BCE LOSS + 一层缓冲层 (BCE sum的情况，先是下降了，然后又上升，然后爆表； mean的情况也一样，没有效果)
  - [X] MSE Loss + 一层缓冲层 (会遇上清零，说明加一层缓冲跟不加没什么差别)
  - [X] Adam + MSE (虽然有点花时间，但是Adam每一步几乎都成功在下降)
  - [X] Adam + BCE (虽然有点花时间，但是Adam每一步几乎都成功在下降)
  - [X] SGD + 0.01 learning rate + BCE (可以, 果然只是对学习率敏感而已)
- [X] 制作数字clustering训练集
- [X] 进行文本clustering
- [X] 乍看之下精度并不高，思索原因，
  - 1）太长的文章，不相关的句子太多，所以大部分的句子都只能将自己的分数弄得很低(sigmoid(0) = 0.5，所以可以得知对于大多数输入，query清零了)，为了抗衡，考虑在训练时不要乘以对角线清零矩阵
  - 2）对于有序输入来说不需要attend过广范围的输入item，这样会导致weight degeneration; 当我们要求model给我们分段的时候，我们应当拥有一个前置知识： 为了分段我应当注目多广的范围，这不能算是作弊
  - 3) 因为使用了transformer类似的架构，所以不能主动遗忘过长的句子（或者底层的BiLSTM已经考虑到了但是效果没有那么好），能否再使用一个BiLSTM来替我完成attention的任务?
- [X] 编码沙漏模型
- [X] 先用helloworld项目来实验, 不乘以对角线清零矩阵(可以，但是优化的速度低了)
- [ ] 晚上回去之前重新用数据集训练，设定epoch = 20 
  - [X] 训练没有diagonal清零的策略
  - [ ] 训练沙漏模型 (TODO)
- [X] 获得实验结果数据
  - Baseline(所有地方都不是分割点) Pk = 0.46275868801249515
  - [X] 确认一下sector是怎么做的
  - [X] epoch = 5, Pk = 0.3914075914222044 (Jan 23 11:14)
  - [X] epoch = 6, Pk = 0.399746192893401 (Jan 24 12:07)
- [X] 决定分割策略: 和上一个集合里所有元素的分数平均值大于平均值 (1.22)
- [X] 编码Pk (1.22)
- [ ] 想办法加速训练
  - 1 epoch 大概3小时15分钟 (Jan 1.24)
  - [ ] 首先最大的性能损耗应该说来自于s-bert, 所以换成256D，然后我们自己训练(性能可能会下降但是总比之前强)
  - [X] batch化 (Doing) (BCE的数值太大，要考虑用mean) (因为是sigmoid之后的结果，首先CE是必须的，但是全部mean的话loss又太小，只能耍一点花招了, 单个batch内sum，batch间手动mean一下就好) (Hello world项目，datas length = 900,  epoch=5. batch=4时候24秒，batch=1时候41秒, batch=8 12秒)
  - [X] 测试时间： 93分钟，跑3907个没有算loss居然要93分钟
  - [X] 测试时间： 15分钟 / 1000 datas (将sbert batch之后)
  - [ ] 将sentences batch了，加速了不少，性能大概提高了25% (不能肯定，之前有可能是因为我在一边用电脑所以影响了性能)
  - [X] 加上了no grad注解, 但是没有什么改变
- [ ] 想办法提高性能
  - [ ] 将输入掉转过来反向训练
  - [ ] 训练沙漏模型
  - [ ] 从头训练s bert
  - [ ] 使用textseg和sector作为baseline
  - [ ] 使用bloom代 s bert
  - [ ] 因为alarm比较多，所以想办法提高分割算法的阈值 (Doing)


## 发现

1. 较好的学习了“列表通常属于同一个项目”
2. 开头一个没见过的名词，通常意味着新Topic的开始




