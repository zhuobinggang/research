# Order Matters(2021/1/8)

这就是重复率和精度都低下的原因

## Done
* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* モジュール化(12/22)
* Backpropagation(誤差逆伝播法)の流れを図示(12/22)
* Hello worldプロジェクトの訓練(12/23)
* Hello worldプロジェクトに基づき、文章分割モデルを構築 (2020/12/28)
* 文章分割モデルを训练 (Failed，训练速度太慢，需要考虑架构，提高训练速度，并且增加参数量，TODO#4) (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* 改良したモデル構造の図示(2021/1/4
* 実験: 一行の学習データには、同じ数字を生成しないように制限する(結果がより悪くなった)(2021/1/4)
* 実験: 学習データ量を増やす（1500、2000、3000）(明らかな改善ではない)(2021/1/4)
* set2seqには順序がとても重要であることを証明(2021/1/8)

## TODO
- [X] #1 Hello worldプロジェクトに基づき、文章分割モデルを構築する (2020/12/28)
- [ ] #2 架构优化
  - [ ] # 2.1 Transformerのアーキテクチャを使用してEncoderを最適化する 
    - [X] # 2.1.1 helloworld项目测试： encoder out 全部加起来完事，for select emb直接输出 (Failed, 效果很差, 正在找原因)
    - [ ] # 2.1.2 找到导致重复输出&精度极低&长度失准的原因
    - [X] # 2.1.3 会不会是训练的时候每次都用正确输出的原因？ (并不是)
    - [X] # 2.1.4 把这篇文章和对应论文读完: Introduction to pointer networks
  - [ ] # 2.1 Decoder使用原论文里的注意力结构，encoder再考虑 (Doing)
  - [ ] # 2.2 同じ出力を繰り返す問題を回避する方法を検討
    - [ ] 将看过的引发想象的论文贴到这里
  - [X] # 2.3 一行の学習データには、同じ数字を生成しないように制限する(Failed，各种数值反而下降了)
  - [ ] # 2.4 生成する学習データの値の上限を上げる
  - [X] # 2.5 学習データ量を増やす(1500, 2000, 3000) (2000, 提高了15%左右的精度, 减低了5%~20%的重复率)
  - [ ] # 2.6 Batch化(首先搞明白为什么要batch化，对于单核机器有无提升) 
  - [ ] # 2.7 尝试将重复率loss化
  - [ ] # 2.8 Using beam search as decoding strategy
  - [ ] # 2.9 パラメータを増やして実験する(現在: 90, 目標: 256, 512)
- [ ] #3 文本分割模型训练 
  - [X] 训练No.1 (Failed，需要考虑架构，提高训练速度，并且增加参数量，TODO#4) (2020/12/28)
- [ ] #4 解释原因: 训练embedding layer的情况(3 vs 6): 随着epoch数量，精度显著提高(0.1个点) (需要阐明原因)
- [X] #5 证明： order matters



- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319) 

## Record

#### 1. hidden state size = 50, No repeated input data, train data rows = 3000, encoded ouput method = add up, train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data
Epoch count: 5, Train time: 94.10896039009094 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.59|0.14|0.42|0.63|
|2|0.67|0.16|0.34|0.6|
|3|0.65|0.21|0.26|0.75|
|4|0.66|0.06|0.46|0.65|
|5|0.67|0.29|0.21|0.69|
|平均|0.65|0.17|0.34|0.66|


#### 2. hidden state size = 200, No repeated input data, train data rows = 3000, encoded ouput method = add up, train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data
Epoch count: 5, Train time: 216.5336332321167 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.32|0.37|0.35|0.93|
|2|0.26|0.74|0.1|0.97|
|3|0.27|0.54|0.33|0.85|
|4|0.44|0.01|0.82|0.68|
|5|0.37|0.42|0.28|0.92|
|平均|0.33|0.42|0.37|0.87|


#### 3. hidden state size = 50, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data
Epoch count: 5, Train time: 114.12675642967224 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.54|0.0|0.02|0.85|
|2|0.62|0.0|0.0|0.81|
|3|0.64|0.0|0.04|0.81|
|4|0.68|0.0|0.0|0.75|
|5|0.69|0.0|0.03|0.73|
|平均|0.63|0.0|0.02|0.79|

#### 4. hidden state size = 50, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = dot(for select, Linear(dh)), test data = train data
Epoch count: 5, Train time: 94.25948977470398 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.6|0.01|0.33|0.74|
|2|0.65|0.04|0.08|0.82|
|3|0.66|0.09|0.02|0.79|
|4|0.72|0.05|0.01|0.76|
|5|0.7|0.04|0.07|0.75|
|平均|0.67|0.05|0.1|0.77|


#### 5. hidden state size = 200, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data
Epoch count: 5, Train time: 209.20178818702698 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.57|0.08|0.26|0.83|
|2|0.58|0.11|0.02|0.83|
|3|0.6|0.02|0.16|0.74|
|4|0.42|0.6|0.04|0.97|
|5|0.56|0.0|0.39|0.73|
|平均|0.55|0.16|0.17|0.82|


#### 6. hidden state size = 50, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data, embedding layer training = True

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.46|0.0|0.1|0.81|
|2|0.6|0.01|0.0|0.88|
|3|0.66|0.0|0.0|0.76|
|4|0.69|0.0|0.0|0.72|
|5|0.71|0.0|0.0|0.69|
|6|0.74|0.01|0.0|0.65|
|7|0.75|0.01|0.0|0.63|
|8|0.77|0.0|0.0|0.59|
|9|0.76|0.0|0.0|0.6|
|10|0.77|0.0|0.0|0.56|
|11|0.75|0.0|0.0|0.6|
|12|0.76|0.0|0.02|0.61|
|13|0.76|0.0|0.07|0.55|
|14|0.77|0.0|0.0|0.59|
|15|0.76|0.0|0.0|0.58|


#### 7. hidden state size = 200, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data, embedding layer training = True

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.46|0.0|0.48|0.77|
|2|0.43|0.0|0.66|0.46|
|3|0.44|0.41|0.0|0.89|
|4|0.43|0.24|0.35|0.81|
|5|0.44|0.0|0.66|0.69|
|6|0.49|0.4|0.07|0.95|
|7|0.53|0.31|0.03|0.88|
|8|0.6|0.04|0.11|0.78|
|9|0.61|0.1|0.1|0.79|
|10|0.59|0.0|0.44|0.62|
|11|0.61|0.01|0.3|0.66|
|12|0.64|0.02|0.13|0.74|
|13|0.4|0.76|0.0|0.98|
|14|0.61|0.07|0.26|0.68|
|15|0.59|0.0|0.44|0.7|
|-|0.64|0.18|0.01|0.77|
|-|0.62|0.15|0.04|0.8|
|-|0.66|0.0|0.21|0.72|
|-|0.54|0.49|0.0|0.88|
|-|0.64|0.0|0.24|0.7|
|-|0.66|0.05|0.06|0.75|
|-|0.69|0.03|0.04|0.74|
|-|0.69|0.03|0.05|0.74|
|-|0.61|0.01|0.41|0.67|
|-|0.63|0.23|0.02|0.84|
|-|0.66|0.01|0.17|0.71|
|-|0.68|0.1|0.05|0.74|
|-|0.6|0.0|0.46|0.63|
|-|0.61|0.0|0.49|0.63|
|-|0.67|0.17|0.01|0.8|
|-|0.64|0.2|0.04|0.82|
|-|0.65|0.09|0.11|0.76|
|-|0.55|0.4|0.07|0.88|
|-|0.67|0.02|0.12|0.75|
|35|0.68|0.01|0.14|0.72|

#### 8. hidden state size = 50, No repeated input data, train data rows = 3000(X2, auto reversed), encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data, embedding layer training = True
Epoch count: 5, Train time: 212.09468483924866 seconds
Epoch count: 5, Train time: 210.56099915504456 seconds
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.64|0.0|0.0|0.82|
|2|0.71|0.0|0.13|0.58|
|3|0.78|0.0|0.0|0.56|
|4|0.82|0.0|0.0|0.52|
|5|0.84|0.0|0.0|0.43|
|6|0.84|0.0|0.0|0.43|
|7|0.76|0.01|0.0|0.61|
|8|0.66|0.27|0.04|0.72|
|9|0.72|0.06|0.02|0.66|
|10|0.56|0.11|0.28|0.72|


#### 思考

然后，确定一下问题点：
1. s2s的问题
2. 指针网络的问题

还有，对比一下sector的双向LSTM做法: 
1. 它的优势在哪？
2. 它用了多少参数？

再次思考，究竟是什么原因导致没有记住之前的输出结果？掉进局部优化坑里了？提高参数量？增加attention会不会有效果？首先是decoder vs decoder, encoder vs encoder, 然后是decoder vs encoder，这样能显著提高网络灵活性才对，理论上, 比如如果我decoder没有attention，就很难记住之前自己做了什么事情，但是decoder，在文本分割的应用上，我觉得不需要attend很早以前的决策才对，那没有记住之前的决策确实是很奇怪的一件事情，就算我让他attend encoder，意思是去找下一个比我大的数么，可是本来就有attend了，为啥还要增加attend？那思考encoder，encoder的输入之间相互attend是我没做的，这样有什么意义呢？比如说输入[7，8，2，5 ]，到后边也许2会attend5，5会attend7，7attend8，我可以输出图来看一下。然后decoder自己attend自己，后边再做，因为大部分重复都是连在一起的，没有可能是因为序列太长的原因。

等等，因为我用的loss是位置准确率，那么中途跳过一个的话，最优解就是重复，这可能就是重复的原因，那么怎么解决这个问题？难道这样的重复理由还不够充足？这样一来，可以猜测的结果是距离相近的答案会渐渐同质化, 变得难以分辨

## 疑难问题集与验证

#### Q: 提高参数量精度没有提升，寻找原因

- [ ] 1. 坐标embedding？
- [X] 2. 会不会是训练的时候每次都用正确输出的原因？ (并不是)
- [X] 3. 对照实验，试试原先的架构下，参数量能否解决该问题? (并不能)
- [X] 4. 首先考虑是训练数据量太少的问题， 尝试#2.5将数据量增加一倍，如果没有效果就结束 (没有效果)
- [X] 5. 获取pointer时的函数f改一下:  (v T tanh(W 1 e j + W 2 d i )) (没有效果)
  - [X] 5.1 确定增加参数能够减少loss (增加参数，精度反而下降了！)
- [ ] 6. decoder带上基于f的attention (doing)
- [ ] 7. Decoding Strategy
- [ ] 8. 能否将重复率作为一个附加loss？
- [ ] 9. Coverage机制
- [ ] 10. Encoer 使用transformer (doing)
- [X] 11. 像原论文一样，将输入限制到0~1之间 (使用可训练embedding来代替了，性能有所提高)
- [X] 12. Order matters(6 vs 8)


## 发现

1. 使用LSTM而非简单add up作为encoder方法，可以显著降低输出长度错误(No.1 vs No.3)
2. LSTM的情况，增加参数量，会降低总体精度，不是很明显； add up的情况，增加参数量，会明显降低精度，实在是神奇 (1 vs 3, 2  vs 4, 6 vs 7)
3. 使用v T tanh(W1(ej) + W2(di))获取for softmax, 无法增加精度(3 vs 4)
4. 训练embedding layer的情况(3 vs 6): 随着epoch数量，精度显著提高(0.1个点) (需要阐明原因)
5. Order matters(6 vs 8)


