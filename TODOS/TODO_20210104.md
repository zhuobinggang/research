## Done
* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* モジュール化(12/22)
* Backpropagation(誤差逆伝播法)の流れを図示(12/22)
* Hello worldプロジェクトの訓練(12/23)
* Hello worldプロジェクトに基づき、文章分割モデルを構築する (2020/12/28)
* 文章分割モデルを训练 (Failed，训练速度太慢，需要考虑架构，提高训练速度，并且增加参数量，TODO#4) (2020/12/28)

## TODO
- [X] #1 Hello worldプロジェクトに基づき、文章分割モデルを構築する (2020/12/28)
- [ ] #2 架构优化
  - [ ] # 2.1 Transformerのアーキテクチャを使用してデコーダを最適化する (Doing)
  - [ ] # 2.2 考虑如何避免重复输出指针的问题
    - [ ] # 2.1.1 假设是s2s的共通问题，查一下解决方法 
  - [X] # 2.3 尝试生成数据的时候限制不能生成重复(Failed，各种数值反而下降了)
  - [ ] # 2.4 尝试生成数据的时候，将数值调大
  - [X] # 2.5 尝试增加数据量(1500, 2000, 3000) (2000, 提高了15%左右的精度, 减低了5%~20%的重复率)
  - [ ] # 2.6 Batch化
  - [ ] # 2.7 尝试将重复率loss化
  - [ ] # 2.8 Using beam search as decoding strategy
- [X] #3 训练 (Failed，需要考虑架构，提高训练速度，并且增加参数量，TODO#4) (2020/12/28)


- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319) 

## Record

#### Hidden state size = 90, train data rows = 900

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.71|0.02|0|0.72|
|10|0.75|0.01|0.01|0.73|
|15|0.78|0|0.01|0.64|
|20|0.83|0.02|0|0.53|
|25|0.80|0.00|0.03|0.54|
|30|0.78|0|0|0.59|

#### Hidden state size = 90, train data rows = 900, No repeated learning data
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.51|0.22|0.31|0.82|
|10|0.48|0.47|0.22|0.84|
|15|0.50|0.36|0.23|0.87|
|20|0.55|0.30|0.30|0.73|
|25|0.56|0.39|0.17|0.82|
|40|0.55|0.2|0.3|0.77|

#### 思考

将重复元素剔除之后精度反而下降了，做几个推测:
1. 首先考虑是训练数据量太少的问题， 尝试#2.5将数据量增加一倍，如果没有效果就结束
2. 考虑一下encoder架构，和transformer对比思考下

然后，确定一下问题点：
1. s2s的问题
2. 指针网络的问题

还有，对比一下sector的双向LSTM做法: 
1. 它的优势在哪？
2. 它用了多少参数？

一个说法： 没能记住刚刚输出的东西
思考： 首先之前的数据当前时刻肯定是拥有的(LSTM的特性)，只是没能学会避开
假设： 能否将重复率作为一个附加loss？

指针网络最初的论文里的配置：
1. a single layer LSTM with either 256 or 512 hidden units
2. stochastic gradient descent

Q: 难道不是只是参数量太少的原因么?
A: 总之先优化架构，增加参数量

Q: Decoding Strategy能用到指针网络上吗？思考一下？
A: ……好像还真可以，可是。。我想想，只是改变decode策略真的有效果么？像Translation任务，首先是有一个高质量的NLU模型，然后才纠结Decoding Strategy… 暂时不考虑把

#### Hidden state size = 90, No repeated learning data, train data rows = 2000
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.59|0.12|0.37|0.79|
|10|0.69|0.03|0.31|0.56|
|15|0.66|0.28|0.05|0.84|
|20|0.62|0.21|0.25|0.72|
|25|0.64|0.02|0.51|0.53|



