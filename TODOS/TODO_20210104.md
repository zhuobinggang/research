## Done
* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* モジュール化(12/22)
* Backpropagation(誤差逆伝播法)の流れを図示(12/22)
* Hello worldプロジェクトの訓練(12/23)
* Hello worldプロジェクトに基づき、文章分割モデルを構築 (2020/12/28)
* 文章分割モデルを训练 (Failed，训练速度太慢，需要考虑架构，提高训练速度，并且增加参数量，TODO#4) (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* 改良したモデル構造の図示(2021/1/4
* 実験: 一行の学習データには、同じ数字を生成しないように制限する(結果がより悪くなった)(2021/1/4)
* 実験: 学習データ量を増やす（1500、2000、3000）(明らかな改善ではない)(2021/1/4)

## TODO
- [X] #1 Hello worldプロジェクトに基づき、文章分割モデルを構築する (2020/12/28)
- [ ] #2 架构优化
  - [ ] # 2.1 Transformerのアーキテクチャを使用してEncoderを最適化する 
    - [X] # 2.1.1 helloworld项目测试： encoder out 全部加起来完事，for select emb直接输出 (Failed, 效果很差, 正在找原因)
    - [ ] # 2.1.2 找到导致重复输出&精度极低&长度失准的原因
    - [X] # 2.1.3 会不会是训练的时候每次都用正确输出的原因？ (并不是)
    - [X] # 2.1.4 把这篇文章和对应论文读完: Introduction to pointer networks
  - [ ] # 2.1 Decoder使用原论文里的注意力结构，encoder再考虑 (Doing)
  - [ ] # 2.2 同じ出力を繰り返す問題を回避する方法を検討
    - [ ] 将看过的引发想象的论文贴到这里
  - [X] # 2.3 一行の学習データには、同じ数字を生成しないように制限する(Failed，各种数值反而下降了)
  - [ ] # 2.4 生成する学習データの値の上限を上げる
  - [X] # 2.5 学習データ量を増やす(1500, 2000, 3000) (2000, 提高了15%左右的精度, 减低了5%~20%的重复率)
  - [ ] # 2.6 Batch化(首先搞明白为什么要batch化，对于单核机器有无提升) 
  - [ ] # 2.7 尝试将重复率loss化
  - [ ] # 2.8 Using beam search as decoding strategy
  - [ ] # 2.9 パラメータを増やして実験する(現在: 90, 目標: 256, 512)
- [ ] #3 文本分割模型训练 
  - [X] 训练No.1 (Failed，需要考虑架构，提高训练速度，并且增加参数量，TODO#4) (2020/12/28)


- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319) 

## Record

#### Hidden state size = 90, train data rows = 900

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.71|0.02|0|0.72|
|10|0.75|0.01|0.01|0.73|
|15|0.78|0|0.01|0.64|
|20|0.83|0.02|0|0.53|
|25|0.80|0.00|0.03|0.54|
|30|0.78|0|0|0.59|

#### Hidden state size = 90, No repeated learning data, train data rows = 2000
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.59|0.12|0.37|0.79|
|10|0.69|0.03|0.31|0.56|
|15|0.66|0.28|0.05|0.84|
|20|0.62|0.21|0.25|0.72|
|25|0.64|0.02|0.51|0.53|


#### Hidden state size = 90, train data rows = 900, No repeated learning data
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.51|0.22|0.31|0.82|
|10|0.48|0.47|0.22|0.84|
|15|0.50|0.36|0.23|0.87|
|20|0.55|0.30|0.30|0.73|
|25|0.56|0.39|0.17|0.82|
|40|0.55|0.2|0.3|0.77|

#### Hidden state size = 90, train data rows = 900, No repeated learning data (No.2)
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.61|0.09|0.34|0.63|
|10|0.56|0.02|0.65|0.52|
|15|0.49|0.47|0.18|0.83|

#### Hidden state size = 200, train data rows = 900, No repeated learning data
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.49|0.46|0.14|0.91|
|10|0.54|0.16|0.4|0.74|

#### hidden state size = 90, No repeated input data, train data rows = 900, encoded ouput method = add up

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.51|0.21|0.42|0.66|
|10|0.49|0.31|0.35|0.7|
|15|0.50|0.20|0.51|0.63|

#### hidden state size = 200, No repeated input data, train data rows = 900, encoded ouput method = add up

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.48|0.47|0.23|0.85|
|10|0.40|0.56|0.23|0.84|

#### hidden state size = 50, No repeated input data, train data rows = 900, encoded ouput method = add up, train method = Decoder do not use correct input

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.39|0.64|0.12|0.89|
|10|0.48|0.29|0.48|0.68|
|15|0.53|0.18|0.40|0.62|
|20|0.50|0.45|0.15|0.82|
|25|0.50|0.47|0.21|0.80|
|30|0.51|0.28|0.31|0.68|


#### hidden state size = 90, No repeated input data, train data rows = 900, encoded ouput method = add up, train method = Decoder do not use correct input

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.47|0.31|0.44|0.65|
|10|0.48|0.39|0.26|0.74|
|15|0.52|0.34|0.25|0.71|
|20|0.47|0.52|0.17|0.85|
|25|0.54|0.29|0.31|0.72|
|30|0.55|0.12|0.53|0.49|


#### hidden state size = 200, No repeated input data, train data rows = 900, encoded ouput method = add up, train method = Decoder do not use correct input

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.47|0.38|0.29|0.81|
|10|0.46|0.5|0.22|0.84|
|15|0.48|0.23|0.45|0.73|
|20|0.53|0.17|0.42|0.69|
|25|0.52|0.18|0.45|0.71|
|30|0.47|0.54|0.13|0.9|

#### hidden state size = 90, No repeated input data, train data rows = 3000, encoded ouput method = add up, train method = Decoder do not use correct input

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.43|0.45|0.33|0.85|
|2|0.44|0.39|0.32|0.75|
|3|0.47|0.41|0.27|0.86|
|4|0.5|0.29|0.39|0.75|
|5|0.49|0.35|0.36|0.81|
|6|0.48|0.32|0.32|0.81|
|7|0.53|0.23|0.42|0.7|
|8|0.47|0.39|0.28|0.87|
|9|0.43|0.5|0.21|0.83|

#### hidden state size = 50, No repeated input data, train data rows = 900, train method = Decoder do not use correct input

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.63|0.02|0.17|0.76|
|10|0.65|0.01|0.19|0.73|
|15|0.64|0.0|0.31|0.73|
|20|0.62|0.08|0.07|0.81|
|25|0.69|0.03|0.03|0.68|
|30|0.67|0.01|0.04|0.69|

#### hidden state size = 90, No repeated input data, train data rows = 900, train method = Decoder do not use correct input

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.39|0.67|0.1|0.96|
|10|0.44|0.56|0.1|0.94|
|15|0.59|0.14|0.38|0.73|
|20|0.56|0.29|0.24|0.79|
|25|0.57|0.0|0.64|0.52|
|30|0.61|0.04|0.6|0.53|

#### hidden state size = 200, No repeated input data, train data rows = 900, train method = Decoder do not use correct input

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.51|0.28|0.37|0.79|
|10|0.48|0.46|0.21|0.82|
|15|0.53|0.07|0.62|0.6|
|20|0.57|0.01|0.63|0.63|
|25|0.27|1.0|0.0|1.0|
|30|0.58|0.01|0.7|0.37|


#### hidden state size = 90, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input

Epoch count: 5, Train time: 136.72463631629944 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.52|0.24|0.29|0.85|
|2|0.54|0.05|0.56|0.67|
|3|0.57|0.27|0.22|0.82|
|4|0.39|0.82|0.03|0.97|
|5|0.57|0.33|0.2|0.81|

#### hidden state size = 90, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di))

Epoch count: 5, Train time: 100.0387670993805 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.46|0.17|0.32|0.78|
|2|0.57|0.0|0.07|0.82|
|3|0.58|0.0|0.06|0.72|
|4|0.63|0.0|0.0|0.81|
|5|0.63|0.02|0.0|0.8|


#### (No.2) hidden state size = 90, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di))

Epoch count: 5, Train time: 100.0387670993805 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.5|0.0|0.41|0.69|
|2|0.54|0.04|0.0|0.9|
|3|0.58|0.0|0.02|0.81|
|4|0.64|0.0|0.03|0.71|
|5|0.56|0.0|0.02|0.84|
|平均|0.56|0.01|0.1|0.79|

#### hidden state size = 90, No repeated input data, train data rows = 3000, encoded ouput method = add up, train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di))

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.53|0.15|0.5|0.8|
|2|0.45|0.2|0.56|0.74|
|3|0.38|0.48|0.28|0.94|
|4|0.53|0.06|0.64|0.73|
|5|0.49|0.36|0.29|0.88|
|平均|0.48|0.25|0.45|0.82|


#### hidden state size = 50, No repeated input data, train data rows = 3000, encoded ouput method = add up, train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data
Epoch count: 5, Train time: 94.10896039009094 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.59|0.14|0.42|0.63|
|2|0.67|0.16|0.34|0.6|
|3|0.65|0.21|0.26|0.75|
|4|0.66|0.06|0.46|0.65|
|5|0.67|0.29|0.21|0.69|
|平均|0.65|0.17|0.34|0.66|


#### hidden state size = 200, No repeated input data, train data rows = 3000, encoded ouput method = add up, train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data
Epoch count: 5, Train time: 216.5336332321167 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.32|0.37|0.35|0.93|
|2|0.26|0.74|0.1|0.97|
|3|0.27|0.54|0.33|0.85|
|4|0.44|0.01|0.82|0.68|
|5|0.37|0.42|0.28|0.92|
|平均|0.33|0.42|0.37|0.87|


#### hidden state size = 50, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data
Epoch count: 5, Train time: 114.12675642967224 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.54|0.0|0.02|0.85|
|2|0.62|0.0|0.0|0.81|
|3|0.64|0.0|0.04|0.81|
|4|0.68|0.0|0.0|0.75|
|5|0.69|0.0|0.03|0.73|
|平均|0.63|0.0|0.02|0.79|

#### hidden state size = 50, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = dot(for select, Linear(dh)), test data = train data
Epoch count: 5, Train time: 94.25948977470398 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.6|0.01|0.33|0.74|
|2|0.65|0.04|0.08|0.82|
|3|0.66|0.09|0.02|0.79|
|4|0.72|0.05|0.01|0.76|
|5|0.7|0.04|0.07|0.75|
|平均|0.67|0.05|0.1|0.77|


#### hidden state size = 200, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di)), test data = train data
Epoch count: 5, Train time: 209.20178818702698 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.57|0.08|0.26|0.83|
|2|0.58|0.11|0.02|0.83|
|3|0.6|0.02|0.16|0.74|
|4|0.42|0.6|0.04|0.97|
|5|0.56|0.0|0.39|0.73|
|平均|0.55|0.16|0.17|0.82|

#### hidden state size = 200, No repeated input data, train data rows = 3000, encoded ouput method = LSTM(Default), train method = Decoder do not use correct input, decoder softmax method = v T tanh(W1(ej) + W2(di))

Epoch count: 5, Train time: 161.04130005836487 seconds

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|1|0.57|0.24|0.15|0.82|
|2|0.47|0.19|0.01|0.85|
|3|0.53|0.04|0.29|0.74|
|4|0.52|0.0|0.56|0.64|
|5|0.44|0.0|0.7|0.47|
|平均|0.5|0.09|0.34|0.7|



#### 思考

然后，确定一下问题点：
1. s2s的问题
2. 指针网络的问题

还有，对比一下sector的双向LSTM做法: 
1. 它的优势在哪？
2. 它用了多少参数？

## 疑难问题集与验证

#### Q: 提高参数量精度没有提升，寻找原因

- [ ] 1. 坐标embedding？
- [X] 2. 会不会是训练的时候每次都用正确输出的原因？ (并不是)
- [X] 3. 对照实验，试试原先的架构下，参数量能否解决该问题? (并不能)
- [X] 4. 首先考虑是训练数据量太少的问题， 尝试#2.5将数据量增加一倍，如果没有效果就结束 (没有效果)
- [X] 5. 获取pointer时的函数f改一下:  (v T tanh(W 1 e j + W 2 d i )) (没有效果)
  - [X] 5.1 确定增加参数能够减少loss (增加参数，精度反而下降了！)
- [ ] 6. decoder带上基于f的attention
- [ ] 7. Decoding Strategy
- [ ] 8. 能否将重复率作为一个附加loss？
- [ ] 9. Coverage机制


## 发现

1. 使用LSTM而非简单add up作为encoder方法，可以显著降低输出长度错误
2. LSTM的情况，增加参数量，会降低总体精度，不是很明显； add up的情况，增加参数量，会明显降低精度，实在是神奇
3. 使用v T tanh(W1(ej) + W2(di))获取for softmax, 无法增加精度



