## Done
* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* モジュール化(12/22)
* Backpropagation(誤差逆伝播法)の流れを図示(12/22)
* Hello worldプロジェクトの訓練(12/23)
* Hello worldプロジェクトに基づき、文章分割モデルを構築する (2020/12/28)
* 文章分割モデルを训练 (Failed，训练速度太慢，需要考虑架构，提高训练速度，并且增加参数量，TODO#4) (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* 改良したモデル構造の図示(2021/1/4
* 実験: 一行の学習データには、同じ数字を生成しないように制限する(結果がより悪くなった)(2021/1/4)
* 実験: 学習データ量を増やす（1500、2000、3000）(明らかな改善ではない)(2021/1/4)

## TODO
- [X] #1 Hello worldプロジェクトに基づき、文章分割モデルを構築する (2020/12/28)
- [ ] #2 架构优化
  - [ ] # 2.1 Transformerのアーキテクチャを使用してデコーダを最適化する (Doing)
    - [X] # 2.1.1 helloworld项目测试： encoder out 全部加起来完事，for select emb直接输出 (Failed, 效果很差, 正在找原因)
    - [ ] # 2.1.2 找到导致重复输出&精度极低&长度失准的原因
  - [ ] # 2.1 Transformer结构获取query vector
  - [ ] # 2.2 考虑如何避免重复输出指针的问题
    - [ ] 将看过的引发想象的论文贴到这里
  - [X] # 2.3 尝试生成数据的时候限制不能生成重复(Failed，各种数值反而下降了)
  - [ ] # 2.4 尝试生成数据的时候，将数值调大
  - [X] # 2.5 尝试增加数据量(1500, 2000, 3000) (2000, 提高了15%左右的精度, 减低了5%~20%的重复率)
  - [ ] # 2.6 Batch化(首先搞明白为什么要batch化，对于单核机器有无提升) (Doing)
  - [ ] # 2.7 尝试将重复率loss化
  - [ ] # 2.8 Using beam search as decoding strategy
  - [ ] # 2.9 パラメータを増やして実験する(現在: 90, 目標: 256, 512)
- [ ] #3 文本分割模型训练 
  - [X] 训练No.1 (Failed，需要考虑架构，提高训练速度，并且增加参数量，TODO#4) (2020/12/28)


- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319) 

## Record

#### Hidden state size = 90, train data rows = 900

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.71|0.02|0|0.72|
|10|0.75|0.01|0.01|0.73|
|15|0.78|0|0.01|0.64|
|20|0.83|0.02|0|0.53|
|25|0.80|0.00|0.03|0.54|
|30|0.78|0|0|0.59|

#### Hidden state size = 90, No repeated learning data, train data rows = 2000
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.59|0.12|0.37|0.79|
|10|0.69|0.03|0.31|0.56|
|15|0.66|0.28|0.05|0.84|
|20|0.62|0.21|0.25|0.72|
|25|0.64|0.02|0.51|0.53|


#### Hidden state size = 90, train data rows = 900, No repeated learning data
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.51|0.22|0.31|0.82|
|10|0.48|0.47|0.22|0.84|
|15|0.50|0.36|0.23|0.87|
|20|0.55|0.30|0.30|0.73|
|25|0.56|0.39|0.17|0.82|
|40|0.55|0.2|0.3|0.77|

#### Hidden state size = 90, train data rows = 900, No repeated learning data (No.2)
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.61|0.09|0.34|0.63|
|10|0.56|0.02|0.65|0.52|
|15|0.49|0.47|0.18|0.83|

#### Hidden state size = 200, train data rows = 900, No repeated learning data
|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.49|0.46|0.14|0.91|
|10|0.54|0.16|0.4|0.74|


### 新架构

#### hidden state size = 90, No repeated input data, train data rows = 900, encoded ouput method = add up

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.51|0.21|0.42|0.66|
|10|0.49|0.31|0.35|0.7|
|15|0.50|0.20|0.51|0.63|

#### hidden state size = 200, No repeated input data, train data rows = 900, encoded ouput method = add up

|Epoch|correct rate|length exceeded rate|length shortened rate|repeat rate|
|----|----|----|----|----|
|5|0.48|0.47|0.23|0.85|
|10|0.40|0.56|0.23|0.84|

### 训练的时候拒绝每一步都用正确数据


#### 思考

将重复元素剔除之后精度反而下降了，做几个推测:
1. 首先考虑是训练数据量太少的问题， 尝试#2.5将数据量增加一倍，如果没有效果就结束
2. 考虑一下encoder架构，和transformer对比思考下

然后，确定一下问题点：
1. s2s的问题
2. 指针网络的问题

还有，对比一下sector的双向LSTM做法: 
1. 它的优势在哪？
2. 它用了多少参数？

一个说法： 没能记住刚刚输出的东西
思考： 首先之前的数据当前时刻肯定是拥有的(LSTM的特性)，只是没能学会避开
假设： 能否将重复率作为一个附加loss？

指针网络最初的论文里的配置：
1. a single layer LSTM with either 256 or 512 hidden units
2. stochastic gradient descent

Q: 难道不是只是参数量太少的原因么?
A: 总之先优化架构，增加参数量

Q: Decoding Strategy能用到指针网络上吗？思考一下？
A: ……好像还真可以，可是。。我想想，只是改变decode策略真的有效果么？像Translation任务，首先是有一个高质量的NLU模型，然后才纠结Decoding Strategy… 暂时不考虑把




## 疑难问题集与假设

#### Q: 提高参数量无法解决encoded ouput method = add up下的精度问题，那么什么因素才能提高精度？

1. 坐标embedding？
2. 会不会是训练的时候每次都用正确输出的原因？
3. 对照实验，试试原先的架构下，参数量能否解决该问题?

#### Q: 重复率居高不下，为什么LSTM没有记住自己的输出？

1. 会不会是训练的时候每次都用正确输出的原因？




