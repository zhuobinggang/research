# 方向转变之后

现在主要的任务就是段落分割了

# TODO

- [X] 编码快速迭代项目, 使用wiki2vec+LSTM, 同时作为对照组
- [X] 找论文，超长sequence怎么处理 
- [ ] 看论文
  - [X] 先看kernel function那篇
  - [ ] 然后继续深究TF are RNN那篇
- [X] 现在问题在于，理论上finetune三次就能用，可是为什么我精度还是那么低？?????????????????
  - [X] 答案就是精度计算方法错了
  - [X] 确定一下，有没有去掉开头的字符？ (有的，在getitem的时候有做)
  - [X] 检验loss函数有没有正确 (大概是正确的)
- [ ] 检验
  - [X] 我之前手动加的SEP有没有被成功转换? 用decode来检验以下 (没有问题，之前没有手动juman那么麻烦)
  - [ ] 更换预处理BERT实验: 黑桥研究室bert+juman vs 东北大bert
  - [ ] 阿尔法值调整实验: 2,1 vs 7,1
  - [ ] 窗口宽度实验: 1 vs 2 vs 3
  - [ ] cross seg 实验: 两边固定数量token embedding vs 两边固定数量句子embedding
- [ ] 思考Topic embedding probing任务
  - [ ] 读完What you can cram into a single `$&!#*` vector: Probing sentence embeddings for linguistic properties (Doing...)
- [ ] 思考能否提高TreeDepth, BShift, SOMO and CoordInv之流的分数(作为研究新方向)
  - [ ] 首先要看看bert有没有用到这个

# Done

- [X] 编码Focus Loss + BERT小说段落分割
  - [X] Focus Loss
    - [X] 弄明白Focus Loss
    - [X] 要对FL和BCE做个对比, 不过是实装时候的事情了
  - [X] 准备数据集
    - [X] 将小节title去掉
  - [X] 实装
    - [X] 将dataset整成什么样子? s, label, s, label ... 这样吗？ 初始化ds的时候就要做手脚，首先确定window size，返回的inpts & labels，inpts: (left ss,  right ss), labels: 0 / 1
    - [X] 通过sbert获取句子embedding
    - [X] 将所有句子embedding存到db里
    - [X] 训练并得到数值
- [X] 看完Long range arena之后开始开始编码
  - [X] 看完Long range arena
  - [X] 首先把对照实验组给编码出来吧，用bert + FL那个
- [X] 实验，用手写Transformer来和之前的BiLSTM对比
  - [X] 看论文考虑用sinsoid还是其他什么方法把位置信息考虑进来 (sinsoid很重要，看看pytorch怎么整)
  - [X] Batch bert
- [X] 实验记录 
  - BERT without FL 全部判断0了，杀你吗 
  - 看来主要是learning rate的原因
- [X] 收集数据集情报: Train(28513:3897), Test(11242:1997)
- [X] 想办法搞定日语Bert
  - [X] 输入句子，输出CLS对应的embedding
  - [X] 检查全部params量，检查可训练params量 (一亿，草，全都可以训练)
- [X] 获取cross seg bert对于test组的结果 (约为0.56的样子)
- [X] 获取cross seg bert对于dev组的结果 (0.48771929824561405, 0.41063515509601184, 0.4458700882117081)
* 数据集情报: Train(28513:3897), Test(11242:1997)

# 实验记录

- [ ] cat sentence vs cross seg (3 epochs) (weighted loss: 3 vs 1)
  - [X] cross seg (0.68) (Tohoku)
  - [X] cross seg (0.62) (Kuro)
  - [X] wiki2vec 1 sentence perside (5 -> 10 epochs)  (epoch = 10, loss = 0.3403270904402808, 0.6645161290322581, 0.5157736604907361, 0.5807724837891176)
- [X] wiki2vec 1 sentence perside vs wiki2vec 2 sentence perside (5 epochs) (如下图)
  - [X] 可能是因为learning rate太小了，2 sentence perside根本没有结果出来, 相比起来1 sentence per side倒是OK,
  - [X] 调高lr再run一次 (不必了0.001已经很高了，再跑多几遍应该就行)
- [X] wiki2vec 1 sentence perside vs wiki2vec 2 sentence perside (5 -> 10 epochs)
  - [X] wiki2vec 2 sentence perside (5 -> 10 epochs) (Nothing happen)
  - [X] 怪事，loss反而上升了？是因为相反原因吗？是因为lr太高？ (调低lr一样没用，很怪)

```
# Tohoku University, Cross Seg, WS = 1
epoch = 4, loss = 0.6033688376640685, 0.6772117962466488, 0.6324486730095142, 0.6540652511651994
epoch = 4, loss = 0.49816578360415503, 0.603401623502126, 0.7816725087631448, 0.681064572425829
epoch = 4, loss = 0.42554458508484827, 0.6211573236889693, 0.6880320480721082, 0.6528866714183891
epoch = 4, loss = 0.3427499571298203, 0.6490140845070422, 0.5768652979469204, 0.6108165429480382

# Kuro, Cross Seg, WS = 1
epoch = 3, loss = 0.5938028532227954, 0.6062152133580705, 0.6544817225838758, 0.6294245124006742
epoch = 3, loss = 0.4994288038963982, 0.5903490759753593, 0.5758637956935403, 0.5830164765525983
epoch = 3, loss = 0.4134285887261803, 0.5951174573929064, 0.6469704556835253, 0.6199616122840691

# Wiki2Vec, Cat Sentence, WS = 1
epoch = 5, loss = 0.6292030956977547, 0, 0.0, 0
epoch = 5, loss = 0.6191255435688476, 0, 0.0, 0
epoch = 5, loss = 0.6048365377610729, 0, 0.0, 0
epoch = 5, loss = 0.5492493520351062, 0.43005181347150256, 0.04156234351527291, 0.07579908675799087
epoch = 5, loss = 0.47191742721378155, 0.4919431279620853, 0.5197796695042564, 0.5054784514243973
epoch = 5, loss = 0.44104245420712535, 0.6725888324873096, 0.2653980971457186, 0.3806104129263914
epoch = 5, loss = 0.41769172540953353, 0.6844050258684405, 0.4636955433149725, 0.5528358208955224
epoch = 5, loss = 0.39382853665157924, 0.6917989417989417, 0.26189283925888834, 0.379949146385761
epoch = 5, loss = 0.3689555295602305, 0.656, 0.4927391086629945, 0.5627680869316557
epoch = 5, loss = 0.3403270904402808, 0.6645161290322581, 0.5157736604907361, 0.5807724837891176
epoch = 5, loss = 0.3049317566279863, 0.5766836445953594, 0.5102653980971458, 0.5414452709883103
epoch = 5, loss = 0.2722113543451527, 0.6117908787541713, 0.5508262393590385, 0.5797101449275361
epoch = 5, loss = 0.24119471178177426, 0.6139534883720931, 0.4626940410615924, 0.5276984580239863
epoch = 5, loss = 0.21443079934958506, 0.5986013986013986, 0.42864296444667, 0.4995622993872192
epoch = 5, loss = 0.18292623168398336, 0.6245954692556634, 0.38657986980470704, 0.47757500773275585
```


# 思考

假设，继续看Fast TF，有什么变化吗？ 
因为序列真的太长，所以不限制窗口是不大可能的，如果限制了窗口，那跟先前研究又有什么区别呢？
如果想要附带前边做过的所有决定，那又有什么意义呢？我们都知道对决策有帮助的只有一小部分区域而已

有没有可能通过wiki2vec来超越呢？

有没有可能通过sector那样的方法做？
怎么安排分类？可不可以说，分类layer，先对前面一个随意分类，如果分割，则后面一句分到对立分类去。
那难道不是跟我之前那个一样。。不对，也许对vector本身的伤害没那么大，会比较好？
总之试试把

跟wiki2vec之前那个作对比
