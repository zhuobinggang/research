# 方向转变之后

现在主要的任务就是段落分割了

# TODO

- [ ] 编码快速迭代项目, 使用wiki2vec+LSTM, 同时作为对照组 (Doing)
- [ ] 找论文，超长sequence怎么处理 (Doing)

# Done

- [X] 编码Focus Loss + BERT小说段落分割
  - [X] Focus Loss
    - [X] 弄明白Focus Loss
    - [X] 要对FL和BCE做个对比, 不过是实装时候的事情了
  - [X] 准备数据集
    - [X] 将小节title去掉
  - [X] 实装
    - [X] 将dataset整成什么样子? s, label, s, label ... 这样吗？ 初始化ds的时候就要做手脚，首先确定window size，返回的inpts & labels，inpts: (left ss,  right ss), labels: 0 / 1
    - [X] 通过sbert获取句子embedding
    - [X] 将所有句子embedding存到db里
    - [X] 训练并得到数值
- [X] 看完Long range arena之后开始开始编码
  - [X] 看完Long range arena
  - [X] 首先把对照实验组给编码出来吧，用bert + FL那个
- [X] 实验，用手写Transformer来和之前的BiLSTM对比
  - [X] 看论文考虑用sinsoid还是其他什么方法把位置信息考虑进来 (sinsoid很重要，看看pytorch怎么整)
  - [X] Batch bert
- [X] 实验记录 
  - BERT without FL 全部判断0了，杀你吗 
  - 看来主要是learning rate的原因
- [X] 收集数据集情报: Train(28513:3897), Test(11242:1997)
- [X] 想办法搞定日语Bert
  - [X] 输入句子，输出CLS对应的embedding
  - [X] 检查全部params量，检查可训练params量 (一亿，草，全都可以训练)
  - [X] 训练1epoch，记录时间 (大约2个半小时1epoch, 显然不利于快速迭代, 如果1epoch之后的结果还行，就暂时放一下，用tough to beat来快速整一波) (1 epoch: Trained! Epochs: 1, Batch size: 8, dataset length: 28513, Time cost: 10018.348648071289 seconds) (Trained! Epochs: 1, Batch size: 8, dataset length: 28513, Time cost: 8215.027804136276 seconds)
  - [X] 测试时间: 15:38 -> 15:49

# 实验记录

- [ ] 实验记录
  - [X] Balanced Loss, [CLS] s1 [SEG] s2, 1 sentence per side
    - 已保存模型以及结果图 (bert BC feb5 epoch8)
    - Balanced Loss(Inverse class frequency)
    - [X] begin: (0.17701476605586194, 0.9964947421131698, 0.30062693556915177)
    - [X] 1 epoch: (0.15575520370040918, 0.8768152228342514, 0.26452148953848476, loss = 0.9729421854071413) (saved: bert BC Feb4 epoch1.tch)
    - [X] 1 -> 8 epoch: loss = [0.8219917761914296, 0.7030498617090974, 0.5837166684519024, 0.4747981751278215, 0.38492678056375046, 0.31751768220432214, 0.26998720271684307, 0.2312763724805752] , results = [(0.150773883650596, 0.8487731597396094, 0.2560616360752323), (0.14783846290695607, 0.8322483725588382, 0.25107636528438704), (0.14187866927592954, 0.7986980470706059, 0.24095475489085277), (0.11492616972069027, 0.6469704556835253, 0.19518090490218293), (0.11439245685821028, 0.6439659489233851, 0.19427449203112018), (0.09019747375911759, 0.5077616424636956, 0.15318377520960796), (0.13734210994484966, 0.7731597396094141, 0.23325024548681925), (0.09766945383383739, 0.5498247371056585, 0.16587355540448673)]
  - [ ] 层级BERT，1 sentence per side 
    - [X] Coding 
    - [ ] 8 epoch train results (Waiting...)
  - [ ] 层级word2vec，1 sentence per side
    - [ ] Coding


# Waiting


# 记录

* 数据集情报: Train(28513:3897), Test(11242:1997)

# 思考

* 现在要做的，为了加快迭代，将bert换成word2vec+LSTM获取句子embedding，统计1epoch的精度，和bert做个简单对比
* 思考，影响实验精度的思考: 
  - 1）[CLS] + [SEG] 的方法应该是无可厚非的，因为谷歌在用，为什么我不能用? 而我的实验和他的实验的唯一区别是，我用一个句子作为输入，而谷歌是用两边固定数量的token来作为输入，因此SEG可以稳定处于中间，这理论上是要影响性能的
  - 2) 如果用交叉分割，就要保证[SEG]在中间，如果用句子CLS token，那就要考虑别的做法, 现在能够想到的是concat然后正常minify
  - 3) 能够快速实现的是2，应当做个实验，单句子实验，9个epoch。期望是：loss 下降的比原来要快，其他数值上升也比较快
  - 4) 3当然要训练，但是这是5号晚上的事情了。因为层级BERT理论上是要比普通bert好的（从谷歌实验可以看出），所以基本上走这条路线没有问题。而这条路的前方是我应该到达的。为了加快迭代，用wiki2vec来代替bert作为实验套装




