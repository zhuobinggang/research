# 方向转变之后

现在主要的任务就是段落分割了

# TODO

- [ ] 编码Focus Loss + BERT小说段落分割
  - [ ] Focus Loss
    - [X] 弄明白Focus Loss
    - [ ] 要对FL和BCE做个对比, 不过是实装时候的事情了
  - [X] 准备数据集
    - [X] 将小节title去掉
  - [ ] 实装
    - [X] 将dataset整成什么样子? s, label, s, label ... 这样吗？ 初始化ds的时候就要做手脚，首先确定window size，返回的inpts & labels，inpts: (left ss,  right ss), labels: 0 / 1
    - [X] 通过sbert获取句子embedding
    - [X] 将所有句子embedding存到db里
    - [ ] 训练并得到数值
- [ ] 看完Long range arena之后开始开始编码
  - [X] 看完Long range arena
  - [ ] 首先把对照实验组给编码出来吧，用bert + FL那个 (Doing)
- [X] 实验，用手写Transformer来和之前的BiLSTM对比
  - [X] 看论文考虑用sinsoid还是其他什么方法把位置信息考虑进来 (sinsoid很重要，看看pytorch怎么整)
  - [X] Batch bert
- [ ] 想办法搞定日语Bert
  - [X] 输入句子，输出CLS对应的embedding
  - [X] 检查全部params量，检查可训练params量 (一亿，草，全都可以训练)
  - [X] 训练1epoch，记录时间 (大约2个半小时1epoch, 显然不利于快速迭代, 如果1epoch之后的结果还行，就暂时放一下，用tough to beat来快速整一波) (1 epoch: Trained! Epochs: 1, Batch size: 8, dataset length: 28513, Time cost: 10018.348648071289 seconds)
    - [ ] 测试时间: from 14:19 -> ?
  - [ ] 减少复杂度，开始一轮训练，先记录目前的正确率，然后记录后面正确率 
    - [X] begin: (0.17701476605586194, 0.9964947421131698, 0.30062693556915177)
- [ ] 实验记录 
  - BERT without FL 全部判断0了，杀你吗 
  - 看来主要是learning rate的原因


# Waiting

- [ ] 实验，用GRU来和之前的BiLSTM对比 
- [ ] 用Twitter Sentiment Analysis数据集实验自己的Transformer, 用LSTM作为BaseLine (效果很差，因为分布很不平均，所以全都输出0了，总之先不看这个了)

# 记录

* 数据集情报: Train(28513:3897), Test(11242:1997)

# 思考

* 用日语bert来整，精度很鸡巴低，想办法解决？要不要用word2vec来整一下？这精度根本没办法用来做实验吧？ 本来是想着，精度差不多就好了紧接着做其他实验就好了，但是这样大的幅度能看得出效果吗？连一个随机模型都打不过，一定是哪里出了问题把？
* 不行，精度太低，浮动太大，计划破产了，说到底，有没有优化到？到底是什么原因？ 有，因为一开始的loss是0.67的, 训练一次跟训练n次并没有什么效果。。。首先是train loss，有下降，但是到0.37左右就停住了，继续改善如何？当然可以，可是基于这样无序（因为使用了mean pool）的东西，就算精度100%，也泛化不过去test那边阿
* 所以说，现在要做的就是搞定正常的transformer操作。搞定之后怎么优化？
* 先找个dataset把...不，就直接用这个，已经够了
* 

