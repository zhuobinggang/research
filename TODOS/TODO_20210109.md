## Done

* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* Hello worldプロジェクトの訓練(12/23)
* 文章分割モデルを構築 (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* (Hello world project)結果セクションの示したように、set2seqには順序がとても重要(Order matters)であることを証明した(2021/1/8)


## TODO

- [X] 不从encoder out回流，不出意外能提高训练速度，如果精度不掉的话就这样做, 
  - 精度掉的很厉害，重复率也高的可怕，速度也没见提高
- [ ] Encoder self attention, 注意比较LSTM和selfattention的速度和精度
- [ ] 提高训练速度
- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319) 
  - [ ] VAE的论文，检讨怎么用到NLP上
- [ ] 解释原因
  - [ ] 训练embedding layer的情况(3 vs 6): 随着epoch数量，精度显著提高(0.1个点) (需要阐明原因)


## Record

### 训练速度记录

