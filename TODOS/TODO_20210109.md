## Done

* Datasetの用意(12/15)
* Pointer networkのHello worldプロジェクトの構築(12/20)
* Hello worldプロジェクトの訓練(12/23)
* 文章分割モデルを構築 (2020/12/28)
* WikiSection Datasetの処理(2021/1/3)
* (Hello world project)結果セクションの示したように、set2seqには順序がとても重要(Order matters)であることを証明した(2021/1/8)


## TODO

- [X] Encoder self attention, 注意比较LSTM和selfattention的速度和精度(精度下降好多不知道为啥, 理论上我的ts encoder没实现错才对阿)
- [X] 训练文章分割模型
  - [X] 倒序训练
  - [X] 不能让它死命输出0
  - [X] 整夜训练，每个epoch都输出记录， (效果很差，需要做一些分析)
- [X] 分析训练结果
  - [X] 检查倒序训练是否有问题
  - [X] 思考： 每次的input居然是单个句子，这是否有点. 确实，怎么办呢...
- [ ] 提高训练速度
- [ ] 论文待看
  - [ ] [关于减轻重复输出的loss function](https://arxiv.org/abs/1908.04319)
  - [ ] VAE的论文，检讨怎么用到NLP上
  - [X] Order matters
- [ ] 解释原因
  - [ ] 训练embedding layer的情况(3 vs 6): 随着epoch数量，精度显著提高(0.1个点) (需要阐明原因)


## 思考


Q: 双向LSTM比指针网络更好的handle这个问题，原因是什么?

1. 指针网络输出是从输入的所有句子中，每次挑选一个句子，它的关注点始终是在“句子”上的，就像是从袋子里每次取出一个球，而不能很好的建模section(复数个句子)。相比之下，双向LSTM，每个句子对应的输出通过LSTM整合了前后文的信息，并据此决定自己归属于哪个部分，就像是给彩虹区分颜色一样而已；
2. 指针网络在训练的时候会去思索，1)被我砍掉的树跟没被砍掉的树究竟有什么区别, 2)被我砍掉的这棵树，跟前面的被我砍掉的树有什么联系; 双向LSTM在训练的时候会去思索，这棵树归属于左边的森林，还是右边的森林；
3. 指针网络更好的handle set2seq问题，双向LSTM更好的handle seq segmentation问题

Q: 花了一个月终于发现弊端，现在前进方向是什么？

1. clustering 
2. 思考能不能在LSTM上加pointer network? (X)
3. 就用Sector来整，创新点就在于分割段落和不需要标签

## Record

### 训练速度记录

