# TODO

首先做日语句子排序。看论文做对比之类。然后从零开始训练BERT。

- [X] 确认日语数据集 (已更新github)
- [ ] 看完谷歌的论文，找用于实验的包括日语的多语言模型
  - [X] 看完谷歌论文
  - [ ] 找用于实验的包括日语的多语言模型
- [ ] 先用小说段落划分作为一个BENCHMARK，简单评价一下
- [ ] 然后搜集句子排序数据集，开始正式评价
- [ ] 编码2018 ACL那个模型，以提供对比
- [ ] 也许需要根据自己的知识做一点改进，以提供对比
- [ ] 写论文

- [X] 逻辑VAE
  - [X] 将潜在空间降为2维
  - [X] 用plt将2维的数据plot出来，不同标签用不同颜色
  - [X] 用我做的VAE，训练之后，将valid数据集合转为潜在空间，然后plot出来看看 (效果很难分辨，因为本身mnist数据就有一点规律)
  - [X] 编码逻辑VAE (理想状态打印出来的结果应该是一条直线的样子，点到点之间应当有明显的线性关系) (嗯。。因为两者平衡还是比较困难的，虽然可以产生联系，但是，嗯，确实可以，可是还是比较麻烦)

- [ ] 论文
  - [ ] Generate sentence from continuous space的包含Japanese的引用论文
  - [ ] VAE + Japanese的论文
  - [ ] VAE的引用论文 + Japanese filter
  - [ ] Probing sentence embeddings for linguistic properties 的引用论文 + 日语filter
  - [ ] 日语多语言BERT模型

- [X] GAN from scratch

- [ ] 对段落分割的一点改进
  - [ ] 分割的时候，同时判断分割点在哪 (加强限制以提高理解力), 先用wiki2vec来试验，如果有效，应该全都有效才对。仔细想想，如果用cross seg，限制了两边的token数量，难道不是减低了大量工作量？
  - [X] 怎么用bert来做crossseg？基于句子分割的锚点，向左右分别拾取足够数量的token
  - [ ] 先用wiki2vec来实验
